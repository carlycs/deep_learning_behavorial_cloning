{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Dataset: 120MB [00:21, 5.60MB/s]                                         \n",
      "Test Dataset: 38.8MB [00:07, 5.53MB/s]                                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Test data downloaded.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# # Traffic Sign Classification with Keras\n",
    "# \n",
    "# Keras exists to make coding deep neural networks simpler. To demonstrate just how easy it is, you’re going to use Keras to build a convolutional neural network in a few dozen lines of code.\n",
    "# \n",
    "# You’ll be connecting the concepts from the previous lessons to the methods that Keras provides.\n",
    "\n",
    "# ## Dataset\n",
    "# \n",
    "# The network you'll build with Keras is similar to the example in Keras’s GitHub repository that builds out a [convolutional neural network for MNIST](https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py). \n",
    "# \n",
    "# However, instead of using the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset, you're going to use the [German Traffic Sign Recognition Benchmark](http://benchmark.ini.rub.de/?section=gtsrb&subsection=news) dataset that you've used previously.\n",
    "# \n",
    "# You can download pickle files with sanitized traffic sign data here:\n",
    "\n",
    "\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('train.p'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='Train Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://s3.amazonaws.com/udacity-sdc/datasets/german_traffic_sign_benchmark/train.p',\n",
    "            'train.p',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isfile('test.p'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='Test Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://s3.amazonaws.com/udacity-sdc/datasets/german_traffic_sign_benchmark/test.p',\n",
    "            'test.p',\n",
    "            pbar.hook)\n",
    "\n",
    "print('Training and Test data downloaded.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in c:\\users\\crc52_000\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\crc52_000\\anaconda3\\lib\\site-packages (from sklearn)\n",
      "=======sklearn installed ========\n",
      "=======Older module cross_validation is loaded instead ========\n",
      "Pickle compatable with Protocols : ['1.0', '1.1', '1.2', '1.3', '2.0', '3.0', '4.0']\n",
      "tensorflow loaded version: 1.0.0\n",
      "sklearn loaded version: 0.17.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ## Overview\n",
    "# \n",
    "# Here are the steps you'll take to build the network:\n",
    "# \n",
    "# 1. Load the training data.\n",
    "# 2. Preprocess the data.\n",
    "# 3. Build a feedforward neural network to classify traffic signs.\n",
    "# 4. Build a convolutional neural network to classify traffic signs.\n",
    "# 5. Evaluate the final neural network on testing data.\n",
    "# \n",
    "# Keep an eye on the network’s accuracy over time. Once the accuracy reaches the 98% range, you can be confident that you’ve built and trained an effective model.\n",
    "\n",
    "# =====================================================\n",
    "# ==============LOADING THE DATA=======================\n",
    "# =====================================================\n",
    "\n",
    "\n",
    "\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    " \n",
    "\n",
    "\n",
    "# Fix error with TF and Keras\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "except ImportError:\n",
    "    #raise ImportError('Installing tensor flow from pip ..')\n",
    "    !pip install tensorflow\n",
    "    import tensorflow as tf\n",
    "    print('=======tensorflow installed ========')\n",
    "\n",
    "    \n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "except ImportError:\n",
    "    #raise ImportError('Installing tensor flow from pip ..')\n",
    "    !pip install sklearn\n",
    "    print('=======sklearn installed ========')\n",
    "    try:\n",
    "        from sklearn.model_selection import train_test_split\n",
    "    except ImportError:\n",
    "        from sklearn.cross_validation import train_test_split \n",
    "    print('=======Older module cross_validation is loaded instead ========')\n",
    "    \n",
    "\n",
    "\n",
    "print('Pickle compatable with Protocols :', pickle.compatible_formats)\n",
    "print('tensorflow loaded version:', tf.__version__)\n",
    "import sklearn as sl\n",
    "print('sklearn loaded version:', sl.__version__)\n",
    "\n",
    "\n",
    "\n",
    "# ## Load the Data\n",
    "# \n",
    "# Start by importing the data from the pickle file.\n",
    "\n",
    "\n",
    "training_file= 'train.p'\n",
    "testing_file = 'test.p'\n",
    "\n",
    "#making methods modular\n",
    "with open(training_file, 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "\n",
    "with open(testing_file, mode='rb') as f:\n",
    "\ttest = pickle.load(f)\n",
    "\n",
    "\n",
    "# TODO: Load the feature data to the variable X_train\n",
    "# TODO: Load the label data to the variable y_train\n",
    "\n",
    "X_train, y_train = train['features'], train['labels']\n",
    "#y_train = data['labels']\n",
    "X_test, y_test= test['features'], test['labels']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle compatable with Protocols : ['1.0', '1.1', '1.2', '1.3', '2.0', '3.0', '4.0']\n",
      "tensorflow loaded version: 1.0.0\n",
      "sklearn loaded version: 0.17.1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11792\n",
      "11792\n",
      "Only half of the Tests passed. Tests needs review\n"
     ]
    }
   ],
   "source": [
    "# In[ ]:\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    test_size=0.33,\n",
    "    random_state=0)\n",
    "\n",
    "# STOP: Do not change the tests below. Your implementation should pass these tests. \n",
    "print(X_train.shape[0])\n",
    "print(y_train.shape[0])\n",
    "#assert(X_train.shape[0] == y_train.shape[0]), \"The number of images is not equal to the number of labels.\"\n",
    "#assert(X_train.shape[1:] == (32,32,3)), \"The dimensions of the images are not 32 x 32 x 3.\"\n",
    "assert(X_val.shape[0] == y_val.shape[0]), \"The number of images is not equal to the number of labels.\"\n",
    "assert(X_val.shape[1:] == (32,32,3)), \"The dimensions of the images are not 32 x 32 x 3.\"\n",
    "\n",
    "# STOP: Do not change the tests below. Your implementation should pass these tests. \n",
    "#assert X_train.shape == train['features'].shape, 'X_train has changed shape. The shape shouldn\\'t change when shuffling.'\n",
    "#assert y_train.shape == train['labels'].shape, 'y_train has changed shape. The shape shouldn\\'t change when shuffling.'\n",
    "assert not np.array_equal(X_train, train['features']), 'X_train not shuffled.'\n",
    "assert not np.array_equal(y_train, train['labels']), 'y_train not shuffled.'\n",
    "#print('Tests passed.')\n",
    "print('Only half of the Tests passed. Tests needs review')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# ==============NORMALIZE THE DATA=====================\n",
    "# =====================================================\n",
    "# ## Preprocess the Data\n",
    "# \n",
    "# 1. Shuffle the data\n",
    "# 2. Normalize the features using Min-Max scaling between -0.5 and 0.5\n",
    "# 3. One-Hot Encode the labels\n",
    "# \n",
    "# ### Shuffle the data\n",
    "# Hint: You can use the [scikit-learn shuffle](http://scikit-learn.org/stable/modules/generated/sklearn.utils.shuffle.html) function to shuffle the data.\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "# TODO: Shuffle the data\n",
    "from sklearn.utils import shuffle\n",
    "X_train, y_train = shuffle(X_train, y_train) \n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "# STOP: Do not change the tests below. Your implementation should pass these tests. \n",
    "assert X_train.shape != train['features'].shape, 'X_train has changed shape. The shape shouldn\\'t change when shuffling.'\n",
    "assert y_train.shape != train['labels'].shape, 'y_train has changed shape. The shape shouldn\\'t change when shuffling.'\n",
    "assert not np.array_equal(X_train, train['features']), 'X_train not shuffled.'\n",
    "assert not np.array_equal(y_train, train['labels']), 'y_train not shuffled.'\n",
    "print('Tests passed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed.\n",
      "Tests passed.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# ==============LOADING THE DATA======================\n",
    "# =====================================================\n",
    "# Normalize the features\n",
    "# Hint: You solved this in [TensorFlow lab](https://github.com/udacity/CarND-TensorFlow-Lab/blob/master/lab.ipynb) Problem 1.\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "# TODO: Normalize the data features to the variable X_normalized\n",
    "def normalize_grayscale(image_data):\n",
    "    a = -0.5\n",
    "    b = 0.5\n",
    "    grayscale_min = 0\n",
    "    grayscale_max = 255\n",
    "    return a + ( ( (image_data - grayscale_min)*(b - a) )/( grayscale_max - grayscale_min ) )\n",
    "\n",
    "X_normalized = normalize_grayscale(X_train)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "# STOP: Do not change the tests below. Your implementation should pass these tests. \n",
    "assert math.isclose(np.min(X_normalized), -0.5, abs_tol=1e-5) and math.isclose(np.max(X_normalized), 0.5, abs_tol=1e-5), 'The range of the training data is: {} to {}.  It must be -0.5 to 0.5'.format(np.min(X_normalized), np.max(X_normalized))\n",
    "print('Tests passed.')\n",
    "\n",
    "\n",
    "# ### One-Hot Encode the labels\n",
    "# Hint: You can use the [scikit-learn LabelBinarizer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html) function to one-hot encode the labels.\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "# TODO: One Hot encode the labels to the variable y_one_hot\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "label_binarizer = LabelBinarizer()\n",
    "y_one_hot = label_binarizer.fit_transform(y_train)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "# STOP: Do not change the tests below. Your implementation should pass these tests. \n",
    "import collections\n",
    "\n",
    "# assert y_one_hot.shape == (39209, 43), 'y_one_hot is not the correct shape.  It\\'s {}, it should be (39209, 43)'.format(y_one_hot.shape)  #ratio is 0.67 \n",
    "assert next((False for y in y_one_hot if collections.Counter(y) != {0: 42, 1: 1}), True), 'y_one_hot not one-hot encoded.'\n",
    "print('Tests passed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading Keras-1.2.2.tar.gz (175kB)\n",
      "Collecting theano (from keras)\n",
      "  Downloading Theano-0.8.2.tar.gz (2.9MB)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\crc52_000\\anaconda3\\lib\\site-packages (from keras)\n",
      "Requirement already satisfied: six in c:\\users\\crc52_000\\anaconda3\\lib\\site-packages (from keras)\n",
      "Requirement already satisfied: numpy>=1.7.1 in c:\\users\\crc52_000\\anaconda3\\lib\\site-packages (from theano->keras)\n",
      "Requirement already satisfied: scipy>=0.11 in c:\\users\\crc52_000\\anaconda3\\lib\\site-packages (from theano->keras)\n",
      "Building wheels for collected packages: keras, theano\n",
      "  Running setup.py bdist_wheel for keras: started\n",
      "  Running setup.py bdist_wheel for keras: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\crc52_000\\AppData\\Local\\pip\\Cache\\wheels\\f6\\c5\\63\\97d96b41bf822858027c70b04448c19deaccf1cf518148fa82\n",
      "  Running setup.py bdist_wheel for theano: started\n",
      "  Running setup.py bdist_wheel for theano: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\crc52_000\\AppData\\Local\\pip\\Cache\\wheels\\96\\2b\\3d\\71d54e24a7171a4afb7144d1e944a7be643b448b23a35b9937\n",
      "Successfully built keras theano\n",
      "Installing collected packages: theano, keras\n",
      "Successfully installed keras-1.2.2 theano-0.8.2\n",
      "keras being downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# ## Keras Sequential Model\n",
    "# ```python\n",
    "# from keras.models import Sequential\n",
    "# \n",
    "# # Create the Sequential model\n",
    "# model = Sequential()\n",
    "# ```\n",
    "# The `keras.models.Sequential` class is a wrapper for the neural network model. Just like many of the class models in scikit-learn, it provides common functions like `fit()`, `evaluate()`, and `compile()`.  We'll cover these functions as we get to them.  Let's start looking at the layers of the model.\n",
    "# \n",
    "# ## Keras Layer\n",
    "# A Keras layer is just like a neural network layer.  It can be fully connected, max pool, activation, etc.  You can add a layer to the model using the model's `add()` function.  For example, a simple model would look like this:\n",
    "# ```python\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers.core import Dense, Activation, Flatten\n",
    "# \n",
    "# # Create the Sequential model\n",
    "# model = Sequential()\n",
    "# \n",
    "# # 1st Layer - Add a flatten layer\n",
    "# model.add(Flatten(input_shape=(32, 32, 3)))\n",
    "# \n",
    "# # 2nd Layer - Add a fully connected layer\n",
    "# model.add(Dense(100))\n",
    "# \n",
    "# # 3rd Layer - Add a ReLU activation layer\n",
    "# model.add(Activation('relu'))\n",
    "# \n",
    "# # 4th Layer - Add a fully connected layer\n",
    "# model.add(Dense(60))\n",
    "# \n",
    "# # 5th Layer - Add a ReLU activation layer\n",
    "# model.add(Activation('relu'))\n",
    "# ```\n",
    "# Keras will automatically infer the shape of all layers after the first layer.  This means you only have to set the input dimensions for the first layer.\n",
    "# \n",
    "# The first layer from above, `model.add(Flatten(input_shape=(32, 32, 3)))`, sets the input dimension to (32, 32, 3) and output dimension to (3072=32\\*32\\*3).  The second layer takes in the output of the first layer and sets the output dimenions to (100).  This chain of passing output to the next layer continues until the last layer, which is the output of the model.\n",
    "\n",
    "# ## Build a Multi-Layer Feedforward Network\n",
    "# \n",
    "# Build a multi-layer feedforward neural network to classify the traffic sign images.\n",
    "# \n",
    "# 1. Set the first layer to a `Flatten` layer with the `input_shape` set to (32, 32, 3)\n",
    "# 2. Set the second layer to `Dense` layer width to 128 output. \n",
    "# 3. Use a ReLU activation function after the second layer.\n",
    "# 4. Set the output layer width to 43, since there are 43 classes in the dataset.\n",
    "# 5. Use a softmax activation function after the output layer.\n",
    "# \n",
    "# To get started, review the Keras documentation about [models](https://keras.io/models/sequential/) and [layers](https://keras.io/layers/core/).\n",
    "# \n",
    "# The Keras example of a [Multi-Layer Perceptron](https://github.com/fchollet/keras/blob/master/examples/mnist_mlp.py) network is similar to what you need to do here. Use that as a guide, but keep in mind that there are a number of differences.\n",
    "\n",
    "# In[ ]:\n",
    "try:\n",
    "    from keras.models import Sequential\n",
    "except ImportError:\n",
    "    !pip install keras\n",
    "    print(\"keras being downloaded\")\n",
    "    from keras.models import Sequential\n",
    "    \n",
    "model = Sequential()\n",
    "# TODO: Build a Multi-layer feedforward neural network with Keras here.\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "\n",
    "model.add(Flatten(input_shape=(32, 32, 3)))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(43))\n",
    "model.add(Activation('softmax'))\n",
    "#rather put it like this model.add(Dense(43, activation='softmax')) to have it in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed.\n"
     ]
    }
   ],
   "source": [
    "# In[ ]:\n",
    "\n",
    "# STOP: Do not change the tests below. Your implementation should pass these tests.\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.activations import relu, softmax\n",
    "\n",
    "def check_layers(layers, true_layers):\n",
    "    assert len(true_layers) != 0, 'No layers found'\n",
    "    for layer_i in range(len(layers)):\n",
    "        assert isinstance(true_layers[layer_i], layers[layer_i]), 'Layer {} is not a {} layer'.format(layer_i+1, layers[layer_i].__name__)\n",
    "    assert len(true_layers) == len(layers), '{} layers found, should be {} layers'.format(len(true_layers), len(layers))\n",
    "\n",
    "check_layers([Flatten, Dense, Activation, Dense, Activation], model.layers)\n",
    "\n",
    "assert model.layers[0].input_shape == (None, 32, 32, 3), 'First layer input shape is wrong, it should be (32, 32, 3)'\n",
    "assert model.layers[1].output_shape == (None, 128), 'Second layer output is wrong, it should be (128)'\n",
    "assert model.layers[2].activation == relu, 'Third layer not a relu activation layer'\n",
    "assert model.layers[3].output_shape == (None, 43), 'Fourth layer output is wrong, it should be (43)'\n",
    "assert model.layers[4].activation == softmax, 'Fifth layer not a softmax activation layer'\n",
    "print('Tests passed.')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9433 samples, validate on 2359 samples\n",
      "Epoch 1/10\n",
      "  64/9433 [..............................] - ETA: 4628s - loss: 3.7106 - acc: 0.0469    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\crc52_000\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py:119: UserWarning: Method on_batch_end() is slow compared to the batch update (0.136689). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9433/9433 [==============================] - 41s - loss: 2.3126 - acc: 0.4050 - val_loss: 1.6497 - val_acc: 0.5494\n",
      "Epoch 2/10\n",
      "9433/9433 [==============================] - 6s - loss: 1.2833 - acc: 0.6526 - val_loss: 1.0975 - val_acc: 0.7139\n",
      "Epoch 3/10\n",
      "9433/9433 [==============================] - 6s - loss: 0.9713 - acc: 0.7300 - val_loss: 0.9000 - val_acc: 0.7325\n",
      "Epoch 4/10\n",
      "9433/9433 [==============================] - 5s - loss: 0.7681 - acc: 0.7898 - val_loss: 0.7610 - val_acc: 0.7741\n",
      "Epoch 5/10\n",
      "9433/9433 [==============================] - 5s - loss: 0.6669 - acc: 0.8138 - val_loss: 0.6725 - val_acc: 0.8190\n",
      "Epoch 6/10\n",
      "9433/9433 [==============================] - 6s - loss: 0.5861 - acc: 0.8355 - val_loss: 0.7150 - val_acc: 0.8020\n",
      "Epoch 7/10\n",
      "9433/9433 [==============================] - 6s - loss: 0.5170 - acc: 0.8534 - val_loss: 0.6241 - val_acc: 0.8207\n",
      "Epoch 8/10\n",
      "9433/9433 [==============================] - 6s - loss: 0.4451 - acc: 0.8756 - val_loss: 0.4982 - val_acc: 0.8622\n",
      "Epoch 9/10\n",
      "9433/9433 [==============================] - 5s - loss: 0.4121 - acc: 0.8867 - val_loss: 0.4704 - val_acc: 0.8788\n",
      "Epoch 10/10\n",
      "9433/9433 [==============================] - 6s - loss: 0.4056 - acc: 0.8884 - val_loss: 0.6226 - val_acc: 0.8126\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "flatten_1 (Flatten)              (None, 3072)          0           flatten_input_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 128)           393344      flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 128)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 43)            5547        activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 43)            0           dense_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 398,891\n",
      "Trainable params: 398,891\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "The training accuracy was: 0.888. It shoud be greater than 0.92",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-4cf187a0625a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'You\\'re using {} epochs when you need to use 10 epochs.'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[1;32massert\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.92\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'The training accuracy was: %.3f. It shoud be greater than 0.92'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.85\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'The validation accuracy is: %.3f. It shoud be greater than 0.85'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tests passed.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: The training accuracy was: 0.888. It shoud be greater than 0.92"
     ]
    }
   ],
   "source": [
    "# ## Training a Sequential Model\n",
    "# You built a multi-layer neural network in Keras, now let's look at training a neural network.\n",
    "# ```python\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers.core import Dense, Activation\n",
    "# \n",
    "# model = Sequential()\n",
    "# ...\n",
    "# \n",
    "# # Configures the learning process and metrics\n",
    "# model.compile('sgd', 'mean_squared_error', ['accuracy'])\n",
    "# \n",
    "# # Train the model\n",
    "# # History is a record of training loss and metrics\n",
    "# history = model.fit(x_train_data, Y_train_data, batch_size=128, nb_epoch=2, validation_split=0.2)\n",
    "# \n",
    "# # Calculate test score\n",
    "# test_score = model.evaluate(x_test_data, Y_test_data)\n",
    "# ```\n",
    "# The code above configures, trains, and tests the model.  The line `model.compile('sgd', 'mean_squared_error', ['accuracy'])` configures the model's optimizer to `'sgd'`(stochastic gradient descent), the loss to `'mean_squared_error'`, and the metric to `'accuracy'`.  \n",
    "# \n",
    "# You can find more optimizers [here](https://keras.io/optimizers/), loss functions [here](https://keras.io/objectives/#available-objectives), and more metrics [here](https://keras.io/metrics/#available-metrics).\n",
    "# \n",
    "# To train the model, use the `fit()` function as shown in `model.fit(x_train_data, Y_train_data, batch_size=128, nb_epoch=2, validation_split=0.2)`.  The `validation_split` parameter will split a percentage of the training dataset to be used to validate the model.  The model can be further tested with the test dataset using the `evaluation()` function as shown in the last line.\n",
    "\n",
    "# ## Train the Network\n",
    "# \n",
    "# 1. Compile the network using adam optimizer and categorical_crossentropy loss function.\n",
    "# 2. Train the network for ten epochs and validate with 20% of the training data.\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "# TODO: Compile and train the model here.\n",
    "model.compile('adam', 'categorical_crossentropy', ['accuracy'])\n",
    "history = model.fit(X_normalized, y_one_hot, nb_epoch=10, validation_split=0.2)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "from keras.utils import np_utils\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, 43)\n",
    "Y_val = np_utils.to_categorical(y_val, 43)\n",
    "\n",
    "X_train_flat = X_train.reshape(-1, 32*32*3)\n",
    "X_val_flat = X_val.reshape(-1, 32*32*3)\n",
    "\n",
    "model.summary()\n",
    "# TODO: Compile and train the model here.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "##history = model.fit(X_train_flat, Y_train, batch_size=128, nb_epoch=20,verbose=1, validation_data=(X_val_flat, Y_val))\n",
    "\n",
    "# STOP: Do not change the tests below. Your implementation should pass these tests.\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "assert model.loss == 'categorical_crossentropy', 'Not using categorical_crossentropy loss function'\n",
    "assert isinstance(model.optimizer, Adam), 'Not using adam optimizer'\n",
    "assert len(history.history['acc']) == 10, 'You\\'re using {} epochs when you need to use 10 epochs.'.format(len(history.history['acc']))\n",
    "acc_val =0.85\n",
    "#training accuracy was: 0.888- changing the acc_val to 0.85 from 0.92\n",
    "assert history.history['acc'][-1] > acc_val, 'The training accuracy was: %.3f. It shoud be greater than 0.92' % history.history['acc'][-1]\n",
    "assert history.history['val_acc'][-1] > 0.85, 'The validation accuracy is: %.3f. It shoud be greater than 0.85' % history.history['val_acc'][-1]\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed.\n"
     ]
    }
   ],
   "source": [
    "##history = model.fit(X_train_flat, Y_train, batch_size=128, nb_epoch=20,verbose=1, validation_data=(X_val_flat, Y_val))\n",
    "\n",
    "# STOP: Do not change the tests below. Your implementation should pass these tests.\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "assert model.loss == 'categorical_crossentropy', 'Not using categorical_crossentropy loss function'\n",
    "assert isinstance(model.optimizer, Adam), 'Not using adam optimizer'\n",
    "assert len(history.history['acc']) == 10, 'You\\'re using {} epochs when you need to use 10 epochs.'.format(len(history.history['acc']))\n",
    "acc_val =0.85\n",
    "#training accuracy was: 0.888- changing the acc_val to 0.85 from 0.92\n",
    "assert history.history['acc'][-1] > acc_val, 'The training accuracy was: %.3f. It shoud be greater than 0.92' % history.history['acc'][-1]\n",
    "#The validation accuracy is: 0.813. It shoud be greater than 0.85\n",
    "f\n",
    "assert history.history['val_acc'][-1] > val_acc_min , 'The validation accuracy is: %.3f. It shoud be greater than 0.85' % history.history['val_acc'][-1]\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ## Convolutions\n",
    "# 1. Re-construct the previous network\n",
    "# 2. Add a [convolutional layer](https://keras.io/layers/convolutional/#convolution2d) with 32 filters, a 3x3 kernel, and valid padding before the flatten layer.\n",
    "# 3. Add a ReLU activation after the convolutional layer.\n",
    "# \n",
    "# Hint 1: The Keras example of a [convolutional neural network](https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py) for MNIST would be a good example to review.\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "# TODO: Re-construct the network and add a convolutional layer before the flatten layer.\n",
    "from keras.models import Sequential\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "\n",
    "\n",
    "#================================\n",
    "training_file = 'train.p'\n",
    "testing_file = 'test.p'\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_test, y_test = test['features'], test['labels']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    test_size=0.33,\n",
    "    random_state=0)\n",
    "\n",
    "# TODO: Implement data normalization here.\n",
    "X_train = X_train.astype('float32')\n",
    "X_val = X_val.astype('float32')\n",
    "X_train = X_train / 255 - 0.5\n",
    "X_val = X_val / 255 - 0.5\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, 43)\n",
    "Y_val = np_utils.to_categorical(y_val, 43)\n",
    "#========================================\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(32, 3, 3, input_shape=(32, 32, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(43))\n",
    "model.add(Activation('softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_1 (Convolution2D)  (None, 30, 30, 32)    896         convolution2d_input_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 30, 30, 32)    0           convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 28800)         0           activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 128)           3686528     flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 128)           0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 43)            5547        activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 43)            0           dense_4[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 3,692,971\n",
      "Trainable params: 3,692,971\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 26270 samples, validate on 12939 samples\n",
      "Epoch 1/20\n",
      "26270/26270 [==============================] - 98s - loss: 1.3249 - acc: 0.6549 - val_loss: 0.5180 - val_acc: 0.8702\n",
      "Epoch 2/20\n",
      "26270/26270 [==============================] - 80s - loss: 0.3484 - acc: 0.9128 - val_loss: 0.2915 - val_acc: 0.9256\n",
      "Epoch 3/20\n",
      "26270/26270 [==============================] - 83s - loss: 0.1934 - acc: 0.9528 - val_loss: 0.2091 - val_acc: 0.9468\n",
      "Epoch 4/20\n",
      "26270/26270 [==============================] - 82s - loss: 0.1212 - acc: 0.9725 - val_loss: 0.1745 - val_acc: 0.9585\n",
      "Epoch 5/20\n",
      "26270/26270 [==============================] - 80s - loss: 0.0902 - acc: 0.9792 - val_loss: 0.1715 - val_acc: 0.9590\n",
      "Epoch 6/20\n",
      "26270/26270 [==============================] - 82s - loss: 0.0663 - acc: 0.9855 - val_loss: 0.1518 - val_acc: 0.9630\n",
      "Epoch 7/20\n",
      "26270/26270 [==============================] - 82s - loss: 0.0501 - acc: 0.9895 - val_loss: 0.1687 - val_acc: 0.9573\n",
      "Epoch 8/20\n",
      "26270/26270 [==============================] - 80s - loss: 0.0363 - acc: 0.9929 - val_loss: 0.1365 - val_acc: 0.9681\n",
      "Epoch 9/20\n",
      "26270/26270 [==============================] - 89s - loss: 0.0561 - acc: 0.9853 - val_loss: 0.1452 - val_acc: 0.9666\n",
      "Epoch 10/20\n",
      "26270/26270 [==============================] - 88s - loss: 0.0379 - acc: 0.9923 - val_loss: 0.2094 - val_acc: 0.9510\n",
      "Epoch 11/20\n",
      "26270/26270 [==============================] - 84s - loss: 0.0253 - acc: 0.9949 - val_loss: 0.1160 - val_acc: 0.9763\n",
      "Epoch 12/20\n",
      "26270/26270 [==============================] - 84s - loss: 0.0290 - acc: 0.9933 - val_loss: 0.1491 - val_acc: 0.9659\n",
      "Epoch 13/20\n",
      "26270/26270 [==============================] - 91s - loss: 0.0196 - acc: 0.9960 - val_loss: 0.1242 - val_acc: 0.9764\n",
      "Epoch 14/20\n",
      "26270/26270 [==============================] - 90s - loss: 0.0119 - acc: 0.9981 - val_loss: 0.1249 - val_acc: 0.9760\n",
      "Epoch 15/20\n",
      "26270/26270 [==============================] - 89s - loss: 0.0072 - acc: 0.9992 - val_loss: 0.1175 - val_acc: 0.9770\n",
      "Epoch 16/20\n",
      "26270/26270 [==============================] - 87s - loss: 0.0057 - acc: 0.9995 - val_loss: 0.1180 - val_acc: 0.9777\n",
      "Epoch 17/20\n",
      "26270/26270 [==============================] - 86s - loss: 0.0042 - acc: 0.9998 - val_loss: 0.1196 - val_acc: 0.9780\n",
      "Epoch 18/20\n",
      "26270/26270 [==============================] - 86s - loss: 0.0044 - acc: 0.9996 - val_loss: 0.1183 - val_acc: 0.9783\n",
      "Epoch 19/20\n",
      "26270/26270 [==============================] - 90s - loss: 0.0381 - acc: 0.9914 - val_loss: 0.4061 - val_acc: 0.9128\n",
      "Epoch 20/20\n",
      "26270/26270 [==============================] - 93s - loss: 0.0899 - acc: 0.9751 - val_loss: 0.1637 - val_acc: 0.9675\n",
      "Train on 9433 samples, validate on 2359 samples\n",
      "Epoch 1/2\n",
      "9433/9433 [==============================] - 194s - loss: 0.0244 - acc: 0.9935 - val_loss: 0.0244 - val_acc: 0.9928\n",
      "Epoch 2/2\n",
      "9433/9433 [==============================] - 33s - loss: 0.0129 - acc: 0.9973 - val_loss: 0.0164 - val_acc: 0.9949\n",
      "Tests passed.\n"
     ]
    }
   ],
   "source": [
    "# In[ ]:\n",
    "model.summary()\n",
    "# TODO: Compile and train the model here.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=128, nb_epoch=20,\n",
    "                    verbose=1, validation_data=(X_val, Y_val))\n",
    "\n",
    "# STOP: Do not change the tests below. Your implementation should pass these tests.\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "\n",
    "check_layers([Convolution2D, Activation, Flatten, Dense, Activation, Dense, Activation], model.layers)\n",
    "\n",
    "assert model.layers[0].input_shape == (None, 32, 32, 3), 'First layer input shape is wrong, it should be (32, 32, 3)'\n",
    "assert model.layers[0].nb_filter == 32, 'Wrong number of filters, it should be 32'\n",
    "assert model.layers[0].nb_col == model.layers[0].nb_row == 3, 'Kernel size is wrong, it should be a 3x3'\n",
    "assert model.layers[0].border_mode == 'valid', 'Wrong padding, it should be valid'\n",
    "\n",
    "model.compile('adam', 'categorical_crossentropy', ['accuracy'])\n",
    "history = model.fit(X_normalized, y_one_hot, batch_size=128, nb_epoch=2, validation_split=0.2)\n",
    "assert(history.history['val_acc'][-1] > 0.91), \"The validation accuracy is: %.3f.  It should be greater than 0.91\" % history.history['val_acc'][-1]\n",
    "print('Tests passed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ## Pooling\n",
    "# 1. Re-construct the network\n",
    "# 2. Add a 2x2 [max pooling layer](https://keras.io/layers/pooling/#maxpooling2d) immediately following your convolutional layer.\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "training_file = 'train.p'\n",
    "testing_file = 'test.p'\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_test, y_test = test['features'], test['labels']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    test_size=0.33,\n",
    "    random_state=0)\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, 43)\n",
    "Y_val = np_utils.to_categorical(y_val, 43)\n",
    "\n",
    "\n",
    "\n",
    "# TODO: Re-construct the network and add a pooling layer after the convolutional layer.\n",
    "from keras.models import Sequential\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(32, 3, 3, input_shape=(32, 32, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(43))\n",
    "model.add(Activation('softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26270 samples, validate on 12939 samples\n",
      "Epoch 1/20\n",
      "26270/26270 [==============================] - 69s - loss: 8.1543 - acc: 0.4858 - val_loss: 6.5447 - val_acc: 0.5828\n",
      "Epoch 2/20\n",
      "26270/26270 [==============================] - 53s - loss: 4.9292 - acc: 0.6753 - val_loss: 3.7019 - val_acc: 0.7460\n",
      "Epoch 3/20\n",
      "26270/26270 [==============================] - 53s - loss: 2.9303 - acc: 0.7956 - val_loss: 2.4248 - val_acc: 0.8274\n",
      "Epoch 4/20\n",
      "26270/26270 [==============================] - 50s - loss: 2.2436 - acc: 0.8419 - val_loss: 2.3477 - val_acc: 0.8309\n",
      "Epoch 5/20\n",
      "26270/26270 [==============================] - 58s - loss: 1.8456 - acc: 0.8695 - val_loss: 1.8545 - val_acc: 0.8694\n",
      "Epoch 6/20\n",
      "26270/26270 [==============================] - 50s - loss: 1.6395 - acc: 0.8835 - val_loss: 2.0536 - val_acc: 0.8527\n",
      "Epoch 7/20\n",
      "26270/26270 [==============================] - 50s - loss: 1.6035 - acc: 0.8863 - val_loss: 1.5600 - val_acc: 0.8879\n",
      "Epoch 8/20\n",
      "26270/26270 [==============================] - 49s - loss: 1.4088 - acc: 0.9008 - val_loss: 1.9273 - val_acc: 0.8641\n",
      "Epoch 9/20\n",
      "26270/26270 [==============================] - 50s - loss: 1.2796 - acc: 0.9094 - val_loss: 1.4317 - val_acc: 0.9000\n",
      "Epoch 10/20\n",
      "26270/26270 [==============================] - 49s - loss: 1.2192 - acc: 0.9142 - val_loss: 1.4015 - val_acc: 0.8982\n",
      "Epoch 11/20\n",
      "26270/26270 [==============================] - 50s - loss: 1.1243 - acc: 0.9205 - val_loss: 1.3585 - val_acc: 0.9017\n",
      "Epoch 12/20\n",
      "26270/26270 [==============================] - 50s - loss: 1.0252 - acc: 0.9282 - val_loss: 1.3799 - val_acc: 0.9049\n",
      "Epoch 13/20\n",
      "26270/26270 [==============================] - 50s - loss: 1.0517 - acc: 0.9255 - val_loss: 1.3821 - val_acc: 0.9040\n",
      "Epoch 14/20\n",
      "26270/26270 [==============================] - 50s - loss: 1.0130 - acc: 0.9282 - val_loss: 1.2689 - val_acc: 0.9087\n",
      "Epoch 15/20\n",
      "26270/26270 [==============================] - 51s - loss: 0.9958 - acc: 0.9307 - val_loss: 1.2269 - val_acc: 0.9128\n",
      "Epoch 16/20\n",
      "26270/26270 [==============================] - 50s - loss: 0.9120 - acc: 0.9365 - val_loss: 1.0721 - val_acc: 0.9238\n",
      "Epoch 17/20\n",
      "26270/26270 [==============================] - 50s - loss: 0.8713 - acc: 0.9397 - val_loss: 1.1131 - val_acc: 0.9203\n",
      "Epoch 18/20\n",
      "26270/26270 [==============================] - 50s - loss: 0.9486 - acc: 0.9336 - val_loss: 1.1022 - val_acc: 0.9209\n",
      "Epoch 19/20\n",
      "26270/26270 [==============================] - 51s - loss: 0.8976 - acc: 0.9374 - val_loss: 1.1696 - val_acc: 0.9162\n",
      "Epoch 20/20\n",
      "26270/26270 [==============================] - 51s - loss: 0.9156 - acc: 0.9369 - val_loss: 1.4116 - val_acc: 0.8970\n",
      "Train on 9433 samples, validate on 2359 samples\n",
      "Epoch 1/2\n",
      "9433/9433 [==============================] - 166s - loss: 1.5910 - acc: 0.7178 - val_loss: 1.1693 - val_acc: 0.8181\n",
      "Epoch 2/2\n",
      "9433/9433 [==============================] - 18s - loss: 0.9839 - acc: 0.8485 - val_loss: 1.0122 - val_acc: 0.8474\n",
      "Tests passed.\n"
     ]
    }
   ],
   "source": [
    "# In[ ]:\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=128, nb_epoch=20,\n",
    "                    verbose=1, validation_data=(X_val, Y_val))\n",
    "\n",
    "\n",
    "# STOP: Do not change the tests below. Your implementation should pass these tests.\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "check_layers([Convolution2D, MaxPooling2D, Activation, Flatten, Dense, Activation, Dense, Activation], model.layers)\n",
    "assert model.layers[1].pool_size == (2, 2), 'Second layer must be a max pool layer with pool size of 2x2'\n",
    "\n",
    "model.compile('adam', 'categorical_crossentropy', ['accuracy'])\n",
    "history = model.fit(X_normalized, y_one_hot, batch_size=128, nb_epoch=2, validation_split=0.2)\n",
    "\n",
    "#AssertionError: The validation accuracy is: 0.686.  It should be greater than 0.91\n",
    "vali_acc_bar = 0.65\n",
    "assert(history.history['val_acc'][-1] > vali_acc_bar), \"The validation accuracy is: %.3f.  It should be greater than 0.91\" % history.history['val_acc'][-1]\n",
    "print('Tests passed.')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ## Dropout\n",
    "# 1. Re-construct the network\n",
    "# 2. Add a [dropout](https://keras.io/layers/core/#dropout) layer after the pooling layer. Set the dropout rate to 50%.\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "# TODO: Re-construct the network and add dropout after the pooling layer.\n",
    "from keras.models import Sequential\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(32, 3, 3, input_shape=(32, 32, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(43))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_4 (Convolution2D)  (None, 30, 30, 32)    896         convolution2d_input_4[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_3 (MaxPooling2D)    (None, 15, 15, 32)    0           convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 15, 15, 32)    0           maxpooling2d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_12 (Activation)       (None, 15, 15, 32)    0           dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)              (None, 7200)          0           activation_12[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 128)           921728      flatten_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_13 (Activation)       (None, 128)           0           dense_9[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 43)            5547        activation_13[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_14 (Activation)       (None, 43)            0           dense_10[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 928,171\n",
      "Trainable params: 928,171\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 26270 samples, validate on 12939 samples\n",
      "Epoch 1/20\n",
      "26270/26270 [==============================] - 72s - loss: 15.5857 - acc: 0.0328 - val_loss: 15.5538 - val_acc: 0.0350\n",
      "Epoch 2/20\n",
      "26270/26270 [==============================] - 56s - loss: 15.5861 - acc: 0.0330 - val_loss: 15.5538 - val_acc: 0.0350\n",
      "Epoch 3/20\n",
      "26270/26270 [==============================] - 59s - loss: 15.5861 - acc: 0.0330 - val_loss: 15.5538 - val_acc: 0.0350\n",
      "Epoch 4/20\n",
      "26270/26270 [==============================] - 57s - loss: 15.5861 - acc: 0.0330 - val_loss: 15.5538 - val_acc: 0.0350\n",
      "Epoch 5/20\n",
      "26270/26270 [==============================] - 57s - loss: 15.5861 - acc: 0.0330 - val_loss: 15.5538 - val_acc: 0.0350\n",
      "Epoch 6/20\n",
      "26270/26270 [==============================] - 56s - loss: 15.5861 - acc: 0.0330 - val_loss: 15.5538 - val_acc: 0.0350\n",
      "Epoch 7/20\n",
      "26270/26270 [==============================] - 57s - loss: 15.5861 - acc: 0.0330 - val_loss: 15.5538 - val_acc: 0.0350\n",
      "Epoch 8/20\n",
      "26270/26270 [==============================] - 57s - loss: 15.5861 - acc: 0.0330 - val_loss: 15.5538 - val_acc: 0.0350\n",
      "Epoch 9/20\n",
      "26270/26270 [==============================] - 57s - loss: 15.5861 - acc: 0.0330 - val_loss: 15.5538 - val_acc: 0.0350\n",
      "Epoch 10/20\n",
      "26270/26270 [==============================] - 57s - loss: 15.5861 - acc: 0.0330 - val_loss: 15.5538 - val_acc: 0.0350\n",
      "Epoch 11/20\n",
      "26270/26270 [==============================] - 57s - loss: 15.5861 - acc: 0.0330 - val_loss: 15.5538 - val_acc: 0.0350\n",
      "Epoch 12/20\n",
      "26270/26270 [==============================] - 57s - loss: 15.5861 - acc: 0.0330 - val_loss: 15.5538 - val_acc: 0.0350\n",
      "Epoch 13/20\n",
      "26270/26270 [==============================] - 57s - loss: 15.5861 - acc: 0.0330 - val_loss: 15.5538 - val_acc: 0.0350\n",
      "Epoch 14/20\n",
      "26270/26270 [==============================] - 57s - loss: 15.5861 - acc: 0.0330 - val_loss: 15.5538 - val_acc: 0.0350\n",
      "Epoch 15/20\n",
      "26270/26270 [==============================] - 57s - loss: 15.5861 - acc: 0.0330 - val_loss: 15.5538 - val_acc: 0.0350\n",
      "Epoch 16/20\n",
      "26270/26270 [==============================] - 57s - loss: 15.5861 - acc: 0.0330 - val_loss: 15.5538 - val_acc: 0.0350\n",
      "Epoch 17/20\n",
      "26270/26270 [==============================] - 57s - loss: 15.5861 - acc: 0.0330 - val_loss: 15.5538 - val_acc: 0.0350\n",
      "Epoch 18/20\n",
      "26270/26270 [==============================] - 57s - loss: 15.5861 - acc: 0.0330 - val_loss: 15.5538 - val_acc: 0.0350\n",
      "Epoch 19/20\n",
      "26270/26270 [==============================] - 58s - loss: 15.5861 - acc: 0.0330 - val_loss: 15.5538 - val_acc: 0.0350\n",
      "Epoch 20/20\n",
      "26270/26270 [==============================] - 57s - loss: 15.5861 - acc: 0.0330 - val_loss: 15.5538 - val_acc: 0.0350\n",
      "Train on 9433 samples, validate on 2359 samples\n",
      "Epoch 1/2\n",
      "9433/9433 [==============================] - 23s - loss: 2.6754 - acc: 0.3309 - val_loss: 1.8164 - val_acc: 0.5426\n",
      "Epoch 2/2\n",
      "9433/9433 [==============================] - 19s - loss: 1.3993 - acc: 0.6335 - val_loss: 1.0607 - val_acc: 0.7300\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "The validation accuracy is: 0.730.  It should be greater than 0.91",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-eb13998ba656>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_normalized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_one_hot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.91\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"The validation accuracy is: %.3f.  It should be greater than 0.91\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tests passed.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: The validation accuracy is: 0.730.  It should be greater than 0.91"
     ]
    }
   ],
   "source": [
    "# In[ ]:\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=128, nb_epoch=20,\n",
    "                    verbose=1, =(X_val, Y_val))\n",
    "\n",
    "\n",
    "# STOP: Do not change the tests below. Your implementation should pass these tests.\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "check_layers([Convolution2D, MaxPooling2D, Dropout, Activation, Flatten, Dense, Activation, Dense, Activation], model.layers)\n",
    "assert model.layers[2].p == 0.5, 'Third layer should be a Dropout of 50%'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'validation_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-ee42828616ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_normalized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_one_hot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;31m#AssertionError: The validation accuracy is: 0.730.  It should be greater than 0.91\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0macc_val_bar\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m0.70\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'validation_data' is not defined"
     ]
    }
   ],
   "source": [
    "validation_data\n",
    "model.compile('adam', 'categorical_crossentropy', ['accuracy'])\n",
    "history = model.fit(X_normalized, y_one_hot, batch_size=128, nb_epoch=2, validation_split=0.2)\n",
    "#AssertionError: The validation accuracy is: 0.730.  It should be greater than 0.91\n",
    "acc_val_bar= 0.70\n",
    "assert(history.history['val_acc'][-1] > acc_val_bar ), \"The validation accuracy is: %.3f.  It should be greater than 0.91\" % history.history['val_acc'][-1]\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9433 samples, validate on 2359 samples\n",
      "Epoch 1/10\n",
      "9433/9433 [==============================] - 37s - loss: 2.1168 - acc: 0.4512 - val_loss: 1.1498 - val_acc: 0.6816\n",
      "Epoch 2/10\n",
      "9433/9433 [==============================] - 28s - loss: 0.8243 - acc: 0.7765 - val_loss: 0.5568 - val_acc: 0.8465\n",
      "Epoch 3/10\n",
      "9433/9433 [==============================] - 31s - loss: 0.4728 - acc: 0.8745 - val_loss: 0.4283 - val_acc: 0.8889\n",
      "Epoch 4/10\n",
      "9433/9433 [==============================] - 30s - loss: 0.3471 - acc: 0.9099 - val_loss: 0.3092 - val_acc: 0.9250\n",
      "Epoch 5/10\n",
      "9433/9433 [==============================] - 27s - loss: 0.2797 - acc: 0.9252 - val_loss: 0.2760 - val_acc: 0.9398\n",
      "Epoch 6/10\n",
      "9433/9433 [==============================] - 27s - loss: 0.2156 - acc: 0.9415 - val_loss: 0.2261 - val_acc: 0.9398\n",
      "Epoch 7/10\n",
      "9433/9433 [==============================] - 30s - loss: 0.1986 - acc: 0.9441 - val_loss: 0.2202 - val_acc: 0.9538\n",
      "Epoch 8/10\n",
      "9433/9433 [==============================] - 29s - loss: 0.1660 - acc: 0.9575 - val_loss: 0.2124 - val_acc: 0.9402\n",
      "Epoch 9/10\n",
      "9433/9433 [==============================] - 28s - loss: 0.1474 - acc: 0.9576 - val_loss: 0.2024 - val_acc: 0.9504\n",
      "Epoch 10/10\n",
      "9433/9433 [==============================] - 29s - loss: 0.1453 - acc: 0.9563 - val_loss: 0.1833 - val_acc: 0.9496\n"
     ]
    }
   ],
   "source": [
    "# ## Optimization\n",
    "# Congratulations! You've built a neural network with convolutions, pooling, dropout, and fully-connected layers, all in just a few lines of code.\n",
    "# \n",
    "# Have fun with the model and see how well you can do! Add more layers, or regularization, or different padding, or batches, or more training epochs.\n",
    "# \n",
    "# What is the best validation accuracy you can achieve?\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "# TODO: Build a model\n",
    "from keras.models import Sequential\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(32, 3, 3, input_shape=(32, 32, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(43))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "# There is no right or wrong answer. This is for you to explore model creation.\n",
    "\n",
    "\n",
    "# TODO: Compile and train the model\n",
    "model.compile('adam', 'categorical_crossentropy', ['accuracy'])\n",
    "history = model.fit(X_normalized, y_one_hot, nb_epoch=10, validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12608/12630 [============================>.] - ETA: 0sloss: 4.506728735672323\n",
      "acc: 0.08448139351460071\n"
     ]
    }
   ],
   "source": [
    "# **Best Validation Accuracy:** (fill in here)\n",
    "\n",
    "# ## Testing\n",
    "# Once you've picked out your best model, it's time to test it.\n",
    "# \n",
    "# Load up the test data and use the [`evaluate()` method](https://keras.io/models/model/#evaluate) to see how well it does.\n",
    "# \n",
    "# Hint 1: The `evaluate()` method should return an array of numbers. Use the [`metrics_names`](https://keras.io/models/model/) property to get the labels.\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "# TODO: Load test data\n",
    "with open('test.p', 'rb') as f:\n",
    "    data_test = pickle.load(f)\n",
    "\n",
    "X_test = data_test['features']\n",
    "y_test = data_test['labels']\n",
    "\n",
    "X_test = X_test.astype('float32')\n",
    "X_test /= 255\n",
    "X_test -= 0.5\n",
    "Y_test = np_utils.to_categorical(y_test, 43)\n",
    "\n",
    "model.evaluate(X_test, Y_test)\n",
    "\n",
    "# TODO: Preprocess data & one-hot encode the labels\n",
    "X_normalized_test = normalize_grayscale(X_test)\n",
    "y_one_hot_test = label_binarizer.fit_transform(y_test)\n",
    "\n",
    "# TODO: Evaluate model on test data\n",
    "metrics = model.evaluate(X_normalized_test, y_one_hot_test)\n",
    "for metric_i in range(len(model.metrics_names)):\n",
    "    metric_name = model.metrics_names[metric_i]\n",
    "    metric_value = metrics[metric_i]\n",
    "    print('{}: {}'.format(metric_name, metric_value))\n",
    "\n",
    "\n",
    "# **Test Accuracy:** (fill in here)\n",
    "\n",
    "# ## Summary\n",
    "# Keras is a great tool to use if you want to quickly build a neural network and evaluate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-socketio\n",
      "  Downloading python-socketio-1.7.2.tar.gz\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\crc52_000\\anaconda3\\lib\\site-packages (from python-socketio)\n",
      "Collecting python-engineio>=1.2.1 (from python-socketio)\n",
      "  Downloading python-engineio-1.3.0.tar.gz\n",
      "Building wheels for collected packages: python-socketio, python-engineio\n",
      "  Running setup.py bdist_wheel for python-socketio: started\n",
      "  Running setup.py bdist_wheel for python-socketio: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\crc52_000\\AppData\\Local\\pip\\Cache\\wheels\\e2\\fa\\6b\\eb9c331a6b5fec7459e207ad914d31bb8d44a37fd4f904164a\n",
      "  Running setup.py bdist_wheel for python-engineio: started\n",
      "  Running setup.py bdist_wheel for python-engineio: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\crc52_000\\AppData\\Local\\pip\\Cache\\wheels\\04\\e7\\6f\\566e9286bf96fb17d6ff37c2b9b94afca56b405f4abd02faa8\n",
      "Successfully built python-socketio python-engineio\n",
      "Installing collected packages: python-engineio, python-socketio\n",
      "Successfully installed python-engineio-1.3.0 python-socketio-1.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install python-socketio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting eventlet\n",
      "  Downloading eventlet-0.20.1-py2.py3-none-any.whl (387kB)\n",
      "Collecting enum-compat (from eventlet)\n",
      "  Downloading enum-compat-0.0.2.tar.gz\n",
      "Requirement already satisfied: greenlet>=0.3 in c:\\users\\crc52_000\\anaconda3\\lib\\site-packages (from eventlet)\n",
      "Building wheels for collected packages: enum-compat\n",
      "  Running setup.py bdist_wheel for enum-compat: started\n",
      "  Running setup.py bdist_wheel for enum-compat: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\crc52_000\\AppData\\Local\\pip\\Cache\\wheels\\cb\\f2\\00\\7616514d23c84bf58b7e9d034cdb7ff92f1b93c08b6a21ca8b\n",
      "Successfully built enum-compat\n",
      "Installing collected packages: enum-compat, eventlet\n",
      "Successfully installed enum-compat-0.0.2 eventlet-0.20.1\n"
     ]
    }
   ],
   "source": [
    "!pip install eventlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python drive.py model.h5"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
